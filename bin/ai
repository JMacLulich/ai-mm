#!/usr/bin/env python3
"""
ai - Multi-model AI tooling for planning, code review, and stabilization

Thin CLI wrapper around the api.py module.

Usage:
    ai plan "prompt" [--model MODEL] [--no-cache]
    ai review [--model MODEL] [--focus AREA] [--no-cache]
    ai stabilize "goal" [--rounds N] [--mode MODE]
    ai cache stats|clear [--older-than HOURS]
    ai usage [--today|--week|--month] [--export FILE]
    ai check-models [--gpt MODEL] [--claude MODEL] [--gemini MODEL] [--local MODEL]

Examples:
    ai plan "Build user authentication"
    ai plan "Migrate auth service" --model mm --depth deep --rounds 3 --strict
    ai plan "Migrate auth service" --quiet-quality
    ai review --model gemini --focus security
    ai review --model mm  # Parallel GPT + Gemini + Claude Opus + local Ollama
    ai stabilize "Add caching layer" --rounds 2 --mode infra
"""

import argparse
import os
import sys
from pathlib import Path

# Setup paths
SCRIPT_DIR = Path(__file__).parent.resolve()
ROOT = SCRIPT_DIR.parent
sys.path.insert(0, str(ROOT / "src"))

# Load environment
def load_env_file(env_path, overwrite=True):
    """Load environment variables from a file."""
    if not env_path.exists():
        return
    with open(env_path) as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith("#") and "=" in line:
                key, value = line.split("=", 1)
                if key.startswith("export "):
                    key = key[7:]
                value = value.strip().strip('"').strip("'")
                if value:
                    if overwrite or key not in os.environ:
                        os.environ[key] = value

# Deterministic env precedence (lowest -> highest):
# 1) Inherited shell env, 2) user config env, 3) repo-local .env
load_env_file(Path.home() / ".config" / "ai-mm" / "env", overwrite=True)
load_env_file(ROOT / ".env", overwrite=True)

# Import API and utilities
from claude_mm.api import plan, review, stabilize  # noqa: E402
from claude_mm.cache import clear_cache, get_cache_stats  # noqa: E402
from claude_mm.config import load_config  # noqa: E402
from claude_mm.config_tui import run_config_tui  # noqa: E402
from claude_mm.models import MODEL_GROUPS  # noqa: E402
from claude_mm.providers.anthropic import AnthropicProvider  # noqa: E402
from claude_mm.providers.google import GoogleProvider  # noqa: E402
from claude_mm.providers.ollama import OllamaProvider  # noqa: E402
from claude_mm.providers.openai import OpenAIProvider  # noqa: E402
from claude_mm.usage import get_usage_stats  # noqa: E402


def cmd_review(args):
    """Handle review command."""
    # Read from stdin
    if not sys.stdin.isatty():
        prompt = sys.stdin.read()
    else:
        print("Error: No input provided. Pipe content to review:", file=sys.stderr)
        print("Example: git diff | ai review", file=sys.stderr)
        sys.exit(1)

    if not prompt.strip():
        print("Error: Empty input", file=sys.stderr)
        sys.exit(1)

    # Handle multimode (mm) and fast groups
    if args.model == "mm":
        models = MODEL_GROUPS["mm"]
        print(
            "ðŸ” Running multimode review with GPT-5.2 + Gemini 3.1 Pro + Claude Opus 4.6 + local Ollama in parallel...",
            file=sys.stderr,
        )
        if args.focus and args.focus != "general":
            print(f"ðŸ“Š Focus: {args.focus}", file=sys.stderr)
        print(file=sys.stderr)
    elif args.model == "all":
        models = MODEL_GROUPS["all"]
        print(
            "ðŸ” Running review with all providers (GPT + Gemini + Opus + local Ollama) in parallel...",
            file=sys.stderr,
        )
        if args.focus and args.focus != "general":
            print(f"ðŸ“Š Focus: {args.focus}", file=sys.stderr)
        print(file=sys.stderr)
    elif args.model == "fast":
        models = MODEL_GROUPS["fast"]
        print(
            "ðŸ” Running review with all 3 fast models (GPT Instant + Gemini Flash + Haiku 4.5) in parallel...",
            file=sys.stderr,
        )
        if args.focus and args.focus != "general":
            print(f"ðŸ“Š Focus: {args.focus}", file=sys.stderr)
        print(file=sys.stderr)
    else:
        models = [args.model] if args.model else None

    # Run review
    try:
        if models and len(models) > 1:
            # Multi-model review
            result = review(
                prompt,
                models=models,
                focus=args.focus,
                use_cache=not args.no_cache,
            )

            # Display results
            for i, (model, res) in enumerate(result):
                if i > 0:
                    print("\n" + "="*80, file=sys.stdout)
                print("="*80, file=sys.stdout)
                print(f"{model.upper()} REVIEW", file=sys.stdout)
                print("="*80, file=sys.stdout)
                print(res.text, file=sys.stdout)

                if not res.cached and res.cost:
                    print(f"\n\nðŸ’° {model.upper()} cost: ${res.cost:.4f}", file=sys.stderr)

            # Show total cost
            if result.total_cost > 0:
                print(f"\n\nðŸ’° Total multimode cost: ${result.total_cost:.4f}", file=sys.stderr)

        else:
            # Single model review
            result = review(
                prompt,
                model=models[0] if models else None,
                focus=args.focus,
                use_cache=not args.no_cache,
            )

            print(result.text, file=sys.stdout)

            if not result.cached and result.cost:
                print(f"\nðŸ’° Actual cost: ${result.cost:.4f}", file=sys.stderr)

    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)


def cmd_plan(args):
    """Handle plan command."""
    # Get goal from args or stdin
    if args.goal:
        goal = args.goal
    elif not sys.stdin.isatty():
        goal = sys.stdin.read()
    else:
        print("Error: No goal provided", file=sys.stderr)
        print("Usage: ai plan \"goal\" or echo \"goal\" | ai plan", file=sys.stderr)
        sys.exit(1)

    if not goal.strip():
        print("Error: Empty goal", file=sys.stderr)
        sys.exit(1)

    # Run plan
    try:
        include_files = None
        if args.include_files:
            include_files = [item.strip() for item in args.include_files.split(",") if item.strip()]

        result = plan(
            goal,
            model=args.model,
            use_cache=not args.no_cache,
            depth=args.depth,
            rounds=args.rounds,
            output_format=args.output_format,
            strict=args.strict,
            context_mode=args.context,
            include_files=include_files,
            confidence_threshold=args.confidence_threshold,
        )

        print(result.text, file=sys.stdout)

        metadata = getattr(result, "metadata", {}) or {}
        confidence = metadata.get("confidence")
        errors = len(metadata.get("errors", []))
        warnings = len(metadata.get("warnings", []))
        rounds = metadata.get("rounds")
        depth = metadata.get("depth")

        if confidence is not None and not args.quiet_quality:
            try:
                confidence_value = float(confidence)
                confidence_text = f"{confidence_value:.2f}"
            except (TypeError, ValueError):
                confidence_text = str(confidence)

            quality_parts = [
                f"confidence {confidence_text}",
                f"errors {errors}",
                f"warnings {warnings}",
            ]
            if rounds is not None:
                quality_parts.append(f"rounds {rounds}")
            if depth:
                quality_parts.append(f"depth {depth}")
            print(f"\nðŸ“ˆ Plan quality: {' | '.join(quality_parts)}", file=sys.stderr)

        if not result.cached and result.cost:
            print(f"\nðŸ’° Actual cost: ${result.cost:.4f}", file=sys.stderr)

    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)


def cmd_stabilize(args):
    """Handle stabilize command."""
    # Get goal from args or stdin
    if args.goal:
        goal = args.goal
    elif not sys.stdin.isatty():
        goal = sys.stdin.read()
    else:
        print("Error: No goal provided", file=sys.stderr)
        print("Usage: ai stabilize \"goal\" or echo \"goal\" | ai stabilize", file=sys.stderr)
        sys.exit(1)

    if not goal.strip():
        print("Error: Empty goal", file=sys.stderr)
        sys.exit(1)

    try:
        result = stabilize(
            goal=goal,
            rounds=args.rounds,
            mode=args.mode,
            use_cache=not args.no_cache,
        )

        print(result["final_plan"].text, file=sys.stdout)

        total_cost = result.get("total_cost", 0)
        if total_cost:
            print(f"\nðŸ’° Actual cost: ${total_cost:.4f}", file=sys.stderr)

    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)


def cmd_cache(args):
    """Handle cache management commands."""
    if args.cache_command == "stats":
        stats = get_cache_stats()
        print("Cache statistics:")
        print(f"  Files: {stats['total_files']}")
        print(f"  Size: {stats['total_size_mb']:.2f} MB")
        if stats['oldest'] is not None:
            from datetime import datetime
            oldest = datetime.fromisoformat(stats['oldest'])
            days_ago = (datetime.now() - oldest).days
            print(f"  Oldest: {days_ago} days ago")

    elif args.cache_command == "clear":
        if args.older_than:
            cleared = clear_cache(older_than_hours=args.older_than)
            print(f"âœ“ Cleared {cleared} cache entries older than {args.older_than} hours")
        else:
            cleared = clear_cache()
            print(f"âœ“ Cleared all {cleared} cache entries")


def cmd_usage(args):
    """Handle usage tracking commands."""
    # Convert period flags to days parameter
    days = None  # all time
    if args.today:
        days = 1
    elif args.week:
        days = 7
    elif args.month:
        days = 30

    stats = get_usage_stats(days=days)

    print("Usage statistics:")
    print(f"  Total cost: ${stats['total_cost']:.4f}")
    print(f"  Total calls: {stats['total_calls']}")

    if stats.get('by_model'):
        print("\n  By model:")
        for model, data in stats['by_model'].items():
            print(f"    {model}: ${data['cost']:.4f} ({data['calls']} calls)")

    if stats.get('by_operation'):
        print("\n  By operation:")
        for op, data in stats['by_operation'].items():
            print(f"    {op}: ${data['cost']:.4f} ({data['calls']} calls)")

    if args.export:
        # Export to CSV
        import csv
        with open(args.export, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['timestamp', 'model', 'operation', 'input_tokens', 'output_tokens', 'cost'])
            for entry in stats.get('entries', []):
                writer.writerow([
                    entry['timestamp'],
                    entry['model'],
                    entry['operation'],
                    entry['input_tokens'],
                    entry['output_tokens'],
                    entry['cost'],
                ])
        print(f"\nâœ“ Exported to {args.export}")


def cmd_config(args):
    """Handle config command - interactive provider config management."""
    try:
        saved = run_config_tui()
        if saved:
            sys.exit(0)
        else:
            sys.exit(1)
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)


def cmd_check_models(args):
    """Check access to configured GPT, Claude, Gemini, and local models."""
    checks = [
        ("gpt", args.gpt, OpenAIProvider),
        ("claude", args.claude, AnthropicProvider),
        ("gemini", args.gemini, GoogleProvider),
        ("local", args.local, OllamaProvider),
    ]
    all_ok = True

    for provider_name, model, provider_cls in checks:
        try:
            provider = provider_cls()
            response = provider.complete(
                prompt="Reply with OK",
                model=model,
                temperature=0,
            )
            text = (response.text or "").strip()
            print(f"{provider_name}/{model}: OK - {text[:80]}")
        except Exception as e:
            all_ok = False
            print(f"{provider_name}/{model}: FAIL - {e}")

    if not all_ok:
        sys.exit(1)


def main():
    """Main CLI entry point."""
    # Load config (available for future extension)
    _ = load_config()

    parser = argparse.ArgumentParser(
        description="Multi-model AI tooling for code review, planning, and stabilization",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    subparsers = parser.add_subparsers(dest="command", help="Command to run")

    # Review command
    review_parser = subparsers.add_parser("review", help="Review code changes")
    review_parser.add_argument(
        "--model",
        help="Model to use (gpt, gemini, claude, opus, ollama, mm, all, fast)",
    )
    review_parser.add_argument(
        "--focus",
        choices=["general", "security", "performance", "architecture", "testing"],
        default="general",
        help="Review focus area",
    )
    review_parser.add_argument("--no-cache", action="store_true", help="Bypass cache")

    # Plan command
    plan_parser = subparsers.add_parser("plan", help="Generate extensive implementation plan")
    plan_parser.add_argument("goal", nargs="?", help="What to plan")
    plan_parser.add_argument("--model", help="Model to use (supports: mm)")
    plan_parser.add_argument(
        "--depth",
        choices=["standard", "deep"],
        default="standard",
        help="Planning depth preset",
    )
    plan_parser.add_argument(
        "--rounds",
        type=int,
        default=2,
        help="Number of critique/revision rounds",
    )
    plan_parser.add_argument(
        "--output-format",
        "--format",
        dest="output_format",
        choices=["markdown", "json"],
        default="markdown",
        help="Plan output format",
    )
    plan_parser.add_argument(
        "--strict",
        action="store_true",
        help="Fail if confidence is low or blocking questions remain",
    )
    plan_parser.add_argument(
        "--confidence-threshold",
        type=float,
        default=0.72,
        help="Minimum confidence required in strict mode",
    )
    plan_parser.add_argument(
        "--context",
        choices=["none", "auto"],
        default="none",
        help="Include additional repository context in planning",
    )
    plan_parser.add_argument(
        "--include-files",
        help="Comma-separated file paths to include as planning context",
    )
    plan_parser.add_argument(
        "--quiet-quality",
        action="store_true",
        help="Hide plan quality summary output on stderr",
    )
    plan_parser.add_argument("--no-cache", action="store_true", help="Bypass cache")

    # Stabilize command
    stab_parser = subparsers.add_parser("stabilize", help="Multi-round plan stabilization")
    stab_parser.add_argument("goal", nargs="?", help="What to plan")
    stab_parser.add_argument("--rounds", type=int, default=2, help="Number of rounds")
    stab_parser.add_argument(
        "--mode",
        choices=["migrations", "docs", "infra"],
        help="Specialized mode",
    )
    stab_parser.add_argument("--no-cache", action="store_true", help="Bypass cache")

    # Cache command
    cache_parser = subparsers.add_parser("cache", help="Manage cache")
    cache_parser.add_argument(
        "cache_command",
        choices=["stats", "clear"],
        help="Cache operation",
    )
    cache_parser.add_argument(
        "--older-than",
        type=int,
        help="Clear entries older than N hours",
    )

    # Usage command
    usage_parser = subparsers.add_parser("usage", help="View usage statistics")
    usage_parser.add_argument("--today", action="store_true", help="Today's usage")
    usage_parser.add_argument("--week", action="store_true", help="This week's usage")
    usage_parser.add_argument("--month", action="store_true", help="This month's usage")
    usage_parser.add_argument("--export", help="Export to CSV file")

    # Config command
    subparsers.add_parser("config", help="Interactive provider configuration")

    # Model access check command
    check_parser = subparsers.add_parser(
        "check-models",
        help="Check access to configured Claude and Gemini models",
    )
    check_parser.add_argument(
        "--gpt",
        default="gpt-5.2-chat-latest",
        help="GPT model to test (default: gpt-5.2-chat-latest)",
    )
    check_parser.add_argument(
        "--claude",
        default="claude-opus-4-6",
        help="Claude model to test (default: claude-opus-4-6)",
    )
    check_parser.add_argument(
        "--gemini",
        default="gemini-3.1-pro-preview",
        help="Gemini model to test (default: gemini-3.1-pro-preview)",
    )
    check_parser.add_argument(
        "--local",
        default="qwen2.5:14b-instruct",
        help="Local Ollama model to test (default: qwen2.5:14b-instruct)",
    )

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        sys.exit(1)

    # Route to command handlers
    if args.command == "review":
        cmd_review(args)
    elif args.command == "plan":
        cmd_plan(args)
    elif args.command == "stabilize":
        cmd_stabilize(args)
    elif args.command == "cache":
        cmd_cache(args)
    elif args.command == "usage":
        cmd_usage(args)
    elif args.command == "config":
        cmd_config(args)
    elif args.command == "check-models":
        cmd_check_models(args)
    else:
        parser.print_help()
        sys.exit(1)


if __name__ == "__main__":
    main()
